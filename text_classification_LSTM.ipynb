{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM for text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that we have divide the practical.csv into two train/test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOADING PACKAGES AND DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': '3', 'text': ['Reuters', '-', 'Short', '-', 'sellers', ',', 'Wall', 'Street', \"'s\", 'dwindling\\\\band', 'of', 'ultra', '-', 'cynics', ',', 'are', 'seeing', 'green', 'again', '.']}\n",
      "Number of training examples: 2525\n",
      "Number of validation examples: 1082\n",
      "Unique tokens in TEXT vocabulary: 12386\n",
      "Unique tokens in LABEL vocabulary: 4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sklearn\n",
    "from torchtext import data, datasets\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "train_csv_path=\"./train.csv\"\n",
    "test_csv_path=\"./train.csv\"\n",
    "\n",
    "TEXT = data.Field(tokenize = 'spacy', include_lengths=True) # Processing the texts into tokens using 'spacy' | To tokenize the document\n",
    "LABEL = data.LabelField(dtype = torch.long)             # Extracting the labels\n",
    "\n",
    "fields=[('label',LABEL), (None,None), ('text', TEXT)]   # To Extract Labels from 1st column and Texts from 3rd column. \n",
    "                                                        # Ignoring 2nd Column due to its irrelevancy\n",
    "                                                        # 'text': Contains List of tokens for each observatons/rows.\n",
    "                                                        # 'label': Denotes the class of each observations/rows.\n",
    "            \n",
    "train_data,test_data = data.TabularDataset.splits(                # Import datasets from the test and train csv files and saved as dictionaries\n",
    "                                                 path = './',     # The datasets created follows the 'field' format\n",
    "                                                 train = train_csv_path,\n",
    "                                                 test = test_csv_path,\n",
    "                                                 format = 'csv',\n",
    "                                                 fields = fields,\n",
    "                                                 skip_header = False)\n",
    "\n",
    "print(vars(train_data.examples[0]))                     # Dictionary of the 1st Sentence\n",
    "\n",
    "train_data,valid_data=train_data.split()                # Splits into train and valid data in a ratio of 8:2\n",
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of validation examples: {len(valid_data)}')\n",
    "\n",
    "MAX_VOCAB_SIZE = 25000\n",
    "TEXT.build_vocab(train_data,                            # To build 'TEXT' Vocabulary, \n",
    "                 max_size = MAX_VOCAB_SIZE,             # Maps the tokens in each rows/obeservations into a vector representation of itself using 'glove dictionary'.\n",
    "                 vectors = \"glove.6B.100d\",             # Converts into a Vector of dimension 100\n",
    "                                                        # For memory efficiency & to find connections between certain sets of words\n",
    "                 unk_init = torch.Tensor.normal_)\n",
    "\n",
    "LABEL.build_vocab(train_data)\n",
    "print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n",
    "print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATING BATCHES TO TRAIN OUR MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torchtext.data.iterator.BucketIterator object at 0x0000028943B71E88>\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64                                         # Define Batch Size\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "train_iterator, valid_iterator = data.BucketIterator.splits((train_data, valid_data),\n",
    "                                                              batch_size = BATCH_SIZE,\n",
    "                                                             device = device,\n",
    "                                                             sort_key= lambda x: len(x.text),\n",
    "                                                             sort_within_batch = True)\n",
    "                                                        # Creating train and validation batches of size 64\n",
    "print(train_iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEFINING OUR LSTM ARCHITECTURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class RNN(nn.Module):\n",
    "     def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers,\n",
    "                  bidirectional, dropout, pad_idx):\n",
    "         super().__init__()\n",
    "\n",
    "         self.embedding = nn.Embedding(vocab_size, embedding_dim,padding_idx=pad_idx)\n",
    "\n",
    "         self.rnn = nn.LSTM(embedding_dim,\n",
    "                             hidden_dim,\n",
    "                            num_layers=n_layers,\n",
    "                             bidirectional=bidirectional,\n",
    "                             dropout=dropout\n",
    "                             )\n",
    "         self.fc = nn.Linear(hidden_dim*2, output_dim)\n",
    "         self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "     def forward(self, text,text_lengths):\n",
    "         #text = [sent len, batch size]\n",
    "         embedded = self.dropout(self.embedding(text))\n",
    "         #embedded = [sent len, batch size, emb dim]\n",
    "\n",
    "         #pack sequence\n",
    "         packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded,text_lengths)\n",
    "         packed_output, (hidden,cell) = self.rnn(packed_embedded)\n",
    "\n",
    "         #hidden = [num layers * num directions, batch size, hid dim]\n",
    "         #cell = [num layers * num directions, batch size, hid dim]\n",
    "\n",
    "         #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden\n",
    "            #layers\n",
    "         #and apply dropout\n",
    "         hidden = self.dropout(torch.cat((hidden[-2,:,:],hidden[-1,:,:]),dim=1))\n",
    "\n",
    "         return self.fc(hidden)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOSS FUNCTION, OPTIMIZER AND DEFINING OF OTHER NECESSARY VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12386, 100])\n",
      "The model has 3,550,796 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = len(LABEL.vocab)\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "model = RNN(INPUT_DIM,\n",
    "             EMBEDDING_DIM,\n",
    "             HIDDEN_DIM,\n",
    "             OUTPUT_DIM,\n",
    "             N_LAYERS,\n",
    "             BIDIRECTIONAL,\n",
    "             DROPOUT,\n",
    "             PAD_IDX)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "print(pretrained_embeddings.shape)\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "\n",
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "\n",
    "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "\n",
    "def count_parameters(model):                               # Total number of parameters that needs to be trained\n",
    "     return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training our LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "Epoch: 1  | time in 0 minutes, 3 seconds\n",
      "\tLoss: 0.5538(train)\t|\tAcc: 38.3%(train)\n",
      "\tLoss: 0.4969(valid)\t|\tAcc: 50.0%(valid)\n",
      "Epoch: 2  | time in 0 minutes, 2 seconds\n",
      "\tLoss: 0.4232(train)\t|\tAcc: 60.3%(train)\n",
      "\tLoss: 0.3593(valid)\t|\tAcc: 66.7%(valid)\n",
      "Epoch: 3  | time in 0 minutes, 1 seconds\n",
      "\tLoss: 0.3405(train)\t|\tAcc: 70.6%(train)\n",
      "\tLoss: 0.2711(valid)\t|\tAcc: 78.1%(valid)\n",
      "Epoch: 4  | time in 0 minutes, 1 seconds\n",
      "\tLoss: 0.2874(train)\t|\tAcc: 75.7%(train)\n",
      "\tLoss: 0.2614(valid)\t|\tAcc: 77.7%(valid)\n",
      "Epoch: 5  | time in 0 minutes, 1 seconds\n",
      "\tLoss: 0.2660(train)\t|\tAcc: 77.2%(train)\n",
      "\tLoss: 0.2593(valid)\t|\tAcc: 78.5%(valid)\n",
      "Epoch: 6  | time in 0 minutes, 1 seconds\n",
      "\tLoss: 0.2430(train)\t|\tAcc: 79.3%(train)\n",
      "\tLoss: 0.2331(valid)\t|\tAcc: 80.6%(valid)\n",
      "Epoch: 7  | time in 0 minutes, 2 seconds\n",
      "\tLoss: 0.2255(train)\t|\tAcc: 82.0%(train)\n",
      "\tLoss: 0.2812(valid)\t|\tAcc: 76.3%(valid)\n",
      "Epoch: 8  | time in 0 minutes, 1 seconds\n",
      "\tLoss: 0.2417(train)\t|\tAcc: 80.9%(train)\n",
      "\tLoss: 0.3265(valid)\t|\tAcc: 70.0%(valid)\n",
      "Epoch: 9  | time in 0 minutes, 2 seconds\n",
      "\tLoss: 0.2312(train)\t|\tAcc: 81.1%(train)\n",
      "\tLoss: 0.2510(valid)\t|\tAcc: 78.3%(valid)\n",
      "Epoch: 10  | time in 0 minutes, 1 seconds\n",
      "\tLoss: 0.1804(train)\t|\tAcc: 85.6%(train)\n",
      "\tLoss: 0.2235(valid)\t|\tAcc: 81.8%(valid)\n",
      "Epoch: 11  | time in 0 minutes, 2 seconds\n",
      "\tLoss: 0.1583(train)\t|\tAcc: 87.2%(train)\n",
      "\tLoss: 0.2113(valid)\t|\tAcc: 83.8%(valid)\n",
      "Epoch: 12  | time in 0 minutes, 1 seconds\n",
      "\tLoss: 0.1443(train)\t|\tAcc: 88.5%(train)\n",
      "\tLoss: 0.2181(valid)\t|\tAcc: 83.5%(valid)\n",
      "Epoch: 13  | time in 0 minutes, 1 seconds\n",
      "\tLoss: 0.1393(train)\t|\tAcc: 88.8%(train)\n",
      "\tLoss: 0.2200(valid)\t|\tAcc: 84.4%(valid)\n",
      "Epoch: 14  | time in 0 minutes, 2 seconds\n",
      "\tLoss: 0.1318(train)\t|\tAcc: 89.9%(train)\n",
      "\tLoss: 0.2103(valid)\t|\tAcc: 84.3%(valid)\n",
      "Epoch: 15  | time in 0 minutes, 2 seconds\n",
      "\tLoss: 0.1065(train)\t|\tAcc: 91.9%(train)\n",
      "\tLoss: 0.2350(valid)\t|\tAcc: 84.7%(valid)\n",
      "Epoch: 16  | time in 0 minutes, 2 seconds\n",
      "\tLoss: 0.0977(train)\t|\tAcc: 93.3%(train)\n",
      "\tLoss: 0.2339(valid)\t|\tAcc: 84.1%(valid)\n",
      "Epoch: 17  | time in 0 minutes, 1 seconds\n",
      "\tLoss: 0.0943(train)\t|\tAcc: 92.5%(train)\n",
      "\tLoss: 0.2205(valid)\t|\tAcc: 84.8%(valid)\n",
      "Epoch: 18  | time in 0 minutes, 2 seconds\n",
      "\tLoss: 0.0772(train)\t|\tAcc: 94.4%(train)\n",
      "\tLoss: 0.2695(valid)\t|\tAcc: 83.4%(valid)\n",
      "Epoch: 19  | time in 0 minutes, 2 seconds\n",
      "\tLoss: 0.0827(train)\t|\tAcc: 94.1%(train)\n",
      "\tLoss: 0.2436(valid)\t|\tAcc: 84.6%(valid)\n",
      "Epoch: 20  | time in 0 minutes, 2 seconds\n",
      "\tLoss: 0.0695(train)\t|\tAcc: 95.2%(train)\n",
      "\tLoss: 0.2335(valid)\t|\tAcc: 84.1%(valid)\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "print(OUTPUT_DIM)\n",
    "\n",
    "losses_eval = []\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "     epoch_loss = 0\n",
    "     epoch_acc = 0\n",
    "\n",
    "     model.eval()\n",
    "\n",
    "     with torch.no_grad():\n",
    "\n",
    "         for batch in iterator:\n",
    "             l=nn.functional.one_hot(batch.label,OUTPUT_DIM)\n",
    "             text, text_lengths = batch.text\n",
    "\n",
    "             predictions = model(text, text_lengths).squeeze(1)\n",
    "\n",
    "             loss = criterion(predictions,l.float())\n",
    "             losses_eval.append(loss.data)\n",
    "             epoch_loss += loss.item()\n",
    "             epoch_acc += (predictions.argmax(1) == batch.label).sum().item()/len(batch.label)\n",
    "\n",
    "     return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "losses_train = []\n",
    "def train(model,iterator, optimizer,criterion):\n",
    "\n",
    "     train_loss=0\n",
    "     train_acc=0\n",
    "\n",
    "     model.train()\n",
    "     for batch in iterator:\n",
    "         l=nn.functional.one_hot(batch.label,OUTPUT_DIM)\n",
    "\n",
    "         optimizer.zero_grad()\n",
    "         text, text_lengths = batch.text\n",
    "         output = model(text,text_lengths).squeeze(1)\n",
    "         loss = criterion(output, l.float())\n",
    "         losses_train.append(loss.data)\n",
    "         train_loss += loss.item()\n",
    "         loss.backward()\n",
    "         optimizer.step()\n",
    "         train_acc += (output.argmax(1) == batch.label).sum().item()/len(batch.label)\n",
    "     return train_loss / len(iterator), train_acc / len(iterator)\n",
    "N_EPO=20\n",
    "import time\n",
    "for epoch in range(N_EPO):\n",
    "     start_time = time.time()\n",
    "     train_loss, train_acc= train(model,train_iterator,optimizer,criterion)\n",
    "     valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "\n",
    "     secs = int(time.time() - start_time)\n",
    "     mins = secs / 60\n",
    "     secs = secs % 60\n",
    "     print('Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(mins, secs))\n",
    "     print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n",
    "     print(f'\\tLoss: {valid_loss:.4f}(valid)\\t|\\tAcc: {valid_acc * 100:.1f}%(valid)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib.colorbar'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-1e2cfb0826f9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mlosses\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlosses_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Training accuracy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcycler\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcycler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolorbar\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrcsetup\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstyle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib.colorbar'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "losses=np.array(losses,dtype=np.float)\n",
    "plt.plot(losses_train)\n",
    "plt.show()\n",
    "print('Training accuracy', train_acc*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses_eval)\n",
    "plt.show()\n",
    "\n",
    "print('prediction accuracy', valid_acc*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
